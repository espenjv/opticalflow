\chapter{The Data Term}
The goal of the penalization of the data term is to make
\begin{align*}
\Upsilon_0(\textbf{w}) = (\nabla f^T \textbf{w} + f_t), 
\end{align*}
as close to zero as possible. This is commonly done by penalizing high values of 
\begin{align*}
\Upsilon_0^2 = (\nabla f^T \textbf{w} + f_t)^2.
\end{align*}
\section{Normalization}
\cite{zimmer2011optic} reported that normalizing the data term can be beneficial. This can be seen from rewriting the squared term above as
\begin{align*}
(\nabla f^T \textbf{w} + f_t)^2 &= \left[ |\nabla f| \left( (\frac{\nabla f^T \textbf{w}}{|\nabla f|} + \frac{f_t}{|\nabla f|} \right) \right]^2  \\
&= |\nabla f|^2 \left[\frac{\nabla f^T}{|\nabla f|} \left(  \textbf{w} + \frac{f_t \nabla f}{|\nabla f|^2} \right) \right]^2 \\
&= |\nabla f|^2 \left[\frac{\nabla f^T}{|\nabla f|} \left( \textbf{w} - \textbf{w}_n \right) \right]^2 ,
\end{align*}
where
\begin{align}
\textbf{w}_n = - \frac{f_t \nabla f}{|\nabla f|^2}
\end{align}
is called the normal flow, which was briefly mentioned in section (\ref{sec:BCA}). Now define 
\begin{align}
d = \frac{\nabla f^T}{|\nabla f|} \left( \textbf{w} - \textbf{w}_n \right),
\end{align}
so that
\begin{align}
\label{Upsilon_dep}
\Upsilon_0(u,v)^2 = |\nabla f|^2 d^2.
\end{align}
Let the flow vector $\textbf{w}$ define a point in the uv-plane. The image gradient $\nabla f$ is then orthogonal to the line defined by $\Upsilon_0(u,v) = 0$. Indeed, for two points $\textbf{w}_1$ and $\textbf{w}_2$, both satisfying $\Upsilon_0(u,v) = 0$,
\begin{align*}
\nabla f^T (\textbf{w}_1-\textbf{w}_2) = f_t - f_t = 0.
\end{align*}
From this one can conclude that $d$ is the distance from the line defined by $\Upsilon_0(u,v) = 0$ and the point $\textbf{w}$ in the uv-plane. A point with $d = 0$ would satisfy the constraint. From this geometric interpretation Zimmer et al. \cite{zimmer2011optic} suggested that one should ideally use $d^2$ to penalize the data term, which is weighted by the square of the image gradient in the expression for $\Upsilon_0^2$,  as seen in (\ref{Upsilon_dep}). Thus the normalized data constraint is
\begin{align}
\label{norm_Upsilon}
\bar{\Upsilon}_0 = \theta_0 (\nabla f^T \textbf{w}+ f_t),
\end{align}
where the normalisation factor $\theta_0$ is defined as
\begin{align*}
\theta_0 = \frac{1}{|\nabla f|^2 + \zeta^2}.
\end{align*}
The regularization parameter $\zeta > 0$ avoids division by zero and simultaneously reduces the effect of small gradients.
\section{The Gradient Constancy Assumption}
In the model so far one has assumed that the illumination is the same for the whole scene, but this assumption is very likely to be violated. Thus, to make the model more robust against additive illumination changes in the image scene [references comes here] proposed to include a constraint regarding the gradients of the brightness. The assumption is called the gradient constancy assumption, and it says that gradients remain constant under their displacement, that is
\begin{align}
\frac{d}{dt}\nabla f(\textbf{r}(t),t)) = 0,
\end{align}
which gives
\begin{align}
\label{gca_constraint}
\nabla f_x \textbf{w} + f_{xt} = 0 & & \text{and} & &  \nabla f_y \textbf{w} + f_{yt} = 0.
\end{align}
To combine the two constancy assumption into one constraint,  define the normalized penalization terms coming from the gradient constancy assumption as
\begin{align}
\bar{\Upsilon}_x(\textbf{w}) = \theta_x (\nabla f_x \textbf{w} + f_{xt}) & & \text{and} & & \bar{\Upsilon}_y(\textbf{w})  = \theta_y (\nabla f_y \textbf{w} + f_{yt}),
\end{align}
where 
\begin{align*}
\theta_x = \frac{1}{|\nabla f_x|^2 + \zeta^2}. & & \text{and} & & \theta_y = \frac{1}{|\nabla f_y|^2 + \zeta^2},
\end{align*}
and let the normalized joint penalization term $\bar{\Upsilon}$ be defined by the following equation
\begin{align*}
\bar{\Upsilon}(\textbf{w})^2 &= \bar{\Upsilon}_0(\textbf{w})^2 + \gamma (\bar{\Upsilon}_x(\textbf{w})^2 + \bar{\Upsilon}_y(\textbf{w})^2) \\
&=  \bar{\Upsilon}_0(\textbf{w})^2 + \gamma \bar{\Upsilon}_{xy}(\textbf{w})^2.
\end{align*}
\section{Robustness Enhancements}
As noted in (\ref{sec:Data_penalization}) the data term can be penalized in different ways. Horn and Schunck \cite{HS} used a quadratic penalizer,
\begin{align}
M(u,v) = \Upsilon(u,v) ^2 = (\nabla f ^T \textbf{w}+ f_t)^2.
\end{align}
but as previously asserted this penalizer has relatively poor robustness against outliers. [Brox et al., 2004 Reference coming soon] used the subquadratic penaliser
\begin{align*}
\Psi_M(s^2) = \sqrt{s^2 + \epsilon^2},
\end{align*} 
with a small regularization parameter $\epsilon > 0$. A separate penalization of the constraints coming from the two constancy assumptions was proposed by [Bruhn and Weickert, 2005], arguing that this would be constructive if one of the assumptions produces an outlier. Using a separate penalization gives the data term
\begin{align}
\label{sep_pen}
M(u,v) = \Psi_M(\bar{\Upsilon}_0^2) + \gamma \Psi_M(\bar{\Upsilon}_{xy}^2).
\end{align}